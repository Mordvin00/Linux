# Операционные системы и виртуализация (Linux)

sudo usermod -aG vboxsf mordvin (Добавить пользователя в групу)
sudo apt install gcc make perl - установка/обновление данных пакетов для установки дополнений гостевой ОС с образа диска виртуальной машины
sudo apt update - проверка обновлений
sudo apt upgrade - обновить
ip a (вывод инфы сетевого подключения)
sudo apt install openssh-server - запуск/проверка сервера
ssh -p 8022 mordvin@localhost - зайти в консоль линукс с винды (NAT)
ssh mordvin@ipAdress - зайти в консоль линукс с винды (Сетевой мост)
sudo ss -ntlp - проверка портов
systemctl status ssh - статус сервера
sudo su - стать админом (exit - вернуться юзером)
df -h - информация файловой системы
ln file1 file3 - абсолютная ссылка с именем file3 (на диске один файл)
ln -s file1 file3 - символическая ссылка на file1 (ярлык с именем file3)
ls - просмотр папки простой
ls -al - просмотр папки в виде списка со скрытыми файлами и папками
ls -ali - просмотр папки в виде списка со скрытыми файлами и папками + inode
ll - псевдоним Линукс просмотра папки в виде списка со скрытыми файлами и папками
pwd - выводит полный путь от корневого каталога к текущему рабочему каталогу
rm - удалить
mv - переименовать/переместить
cp - копировать
mkdir - создать папку
mkdir -p - создать папку  (-p, --parents не выдавать ошибку, если существует; создавать родительские каталоги при необходимости)

"команда" --help справка по команде

touch filename - создать пустой файл
cat > filename - создать файл и ввести в него текст
cat >> filename - добавить текст в файл
cat filename - вывод текста файла на экран
Для безопасного удаления пустых папок удобно также использовать команду rmdir
Файл также можно создать командой:
echo "text" >> filename

sudo chmod u=rw,g=rw,o=r filename - права доступа файла
sudo chmod 664 filename - права доступа файла с помощью восьмеричной системы (цифровой код с набором прав доступа)

sudo chown -R www-data:sambashare foldername - задать владельца и группу для папки (владелец:группа)

sudo useradd -s /bin/bash username - создание пользователя без вопросов но с заданными параметрами
sudo adduser username - создание пользователя )система предложит вводить данные пользователя)

id - вывод информации текущего пользователя
id username - вывод информации указанного пользователя

sudo userdel -r username - удаление пользователя (-r этот параметр так же удалит его рабочую папку)

sudo addgroup gropname - создание группы
sudo groupdel gropname - удаление группы

sudo usermod -g gropname username - смена основной группы пользователя
sudo usermod -aG gropname username - добавление пользователя в группу

cat /etc/passwd | grep username - вывод инфы с файла пользователей ( | grep username - фильтрация)

sudo usermod -aG sudo username - дать права sudo пользователю

sudo visudo - прописать непосредственно пользователя в sudo в файле - /etc/sudoers (можно сделать неограниченные права на уровне root, убрать запрос пароля (ALL=(ALL:ALL) NOPASSWD: ALL - перед последним ALL) и т.д.), но лучше сюда не лазить и давать права на уровне групп!

sudo visudo
username    ALL=(ALL:ALL) NOPASSWD: /bin/bash, /usr/bin/passwd - права sudo на несколько команд

which passwd - где находится команда на примере passwd - /usr/bin/passwd

https://dev.mysql.com/downloads/repo/apt/ - репозиторий Ubuntu / Debian (Architecture Independent), DEB Package

wget https://dev.mysql.com/get/mysql-apt-config_0.8.30-1_all.deb - скачать пакет в текущую папку

sudo dpkg -i mysql-apt-config_0.8.30-1_all.deb - установка скачанного пакета

cd /etc/apt - папка с пакетами

cat mysql.list - вывод содержимого списка репозиториев в файле

sudo apt update - скачать списки пакетов с репозитория

sudo apt install mysql-community-server - установка необходимого с выбранного репозитория

sudo apt purge mysql-community-server - удаление выбранного пакета

sudo apt remove mysql-community-server - удаление выбранного пакета, но файлы настроек конфига останутся

sudo dpkg -r mysql-apt-config - удаление установленного пакета
Удалять пакеты в команде dpkg можно также с ключом "-P|--purge".
Этот вариант кроме удаления исходников чистит и файлы конфигурации удаляемого пакета.

sudo snap install --classic certbot - установка snap-пакета

sudo snap remove certbot - удаление snap-пакета

sudo add-apt-repository ppa:obsproject/obs-studio - подключение PPA репозитория

sudo apt install obs-studio - установка пакета из PPA репозитория

В Linux также есть планировщик задач 'at'.
Информацию можно посмотреть тут:
https://andreyex.ru/linux/komandy-linux-i-komandy-shell/komanda-at-v-linux/

nano /etc/crontab - редактирование системного файла расписания
Выполнение регулярных задач по расписанию
Автоматизация обслуживания системы или приложений
Системные задачи:
/etc/crontab
/etc/cron.d/*
Пользовательские задачи:
/var/spool/cron/*
управление: утилита crontab

crontab - l вывести содержимое текущего файла расписания
crontab -r удаление текущего файла расписания
crontab -e редактирование текущего файла расписания
sudo crontab -u username – работа с файлом расписания другого пользователя

shutdown -P --no-wall - выключить с терминала без вопросов
shutdown --help
shutdown [OPTIONS...] [TIME] [WALL...]

Shut down the system.

Options:
     --help      Show this help
  -H --halt      Halt the machine
  -P --poweroff  Power-off the machine
  -r --reboot    Reboot the machine
  -h             Equivalent to --poweroff, overridden by --halt
  -k             Don't halt/power-off/reboot, just send warnings
     --no-wall   Don't send wall message before halt/power-off/reboot
  -c             Cancel a pending shutdown
See the shutdown(8) man page for details.

## Завершить процесс можно командой kill. Справка по команде: 'man kill'.

ip a - вывести параметры сети на экран
ip -c a - вывести параметры сети на экран с цветовым подсвечиванием
параметр -s ещё и выдат статистику по интерфейсам
ip r - вывести параметры маршртизации на экран, так же можно с параметром -c

ss -ntlp - паказать набор работающих портов или сокетов
ss – socket stat
ss -ntlp – TCP-сокеты в состоянии LISTEN
ss -ntulp – TCP и UDP-сокеты в состоянии LISTEN
ss -tulpan – Все TCP и UDP-сокеты

ip addr add 10.0.2.15/24 broadcast 10.0.2.255 dev enp0s3 или
ip addr add 10.0.2.15/255.255.255.0 broadcast 10.0.2.255 dev enp0s3 - добавление ip адреса
ip addr del 10.0.2.16/24 broadcast 10.0.2.255 dev enp0s3 - удаление ip адреса

ip route add default via 10.0.2.1 dev enp0s3 - добавить маршрут по умолчанию (шлюз)
если не на правах root, то не забываем перед командами прописывать sudo

ip route del default via 10.0.2.1 dev enp0s3 - удаление указанного маршрута

cd /etc/netplan/ - каталог с файлами конфигурации сети

nano 01-network-manager-all.yaml - правка файла конфигурации сети

netplan try - применение настроек сети с подтверждением, если связь потеряется через 2 минуты вернутся предыдущие.

ping ya.ru - проверка связи с хостом

resolvectl dns - показать использующиеся DNS

Информаия о хосте:
host -t a ya.ru
host -t a ya.ru 1.1.1.1
host -t mx ya.ru
ya.ru mail is handled by 10 mx.yandex.ru.
dig ya.ru
tracepath ya.ru

iptables -nvL - информаия маршрутизаии
iptables -nvL --line-numbers - информаия маршрутизаии с номерами строк

iptables -nvL -t nat - информаия маршрутизаии NAT

## Пример конфигурации сервера
# SSH allow
iptables -A INPUT -p tcp --dport=22 -j ACCEPT
# HTTP, HTTPS allow
iptables -A INPUT -p tcp -m multiport --dport 80,443 -j ACCEPT
# loopback allow
iptables -A INPUT -i lo -j ACCEPT
# ICMP
iptables -A INPUT -p icmp -j ACCEPT
# established connections allow
iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# policy drop
iptables -P INPUT DROP
-A - конец списка
-I - начало списка
Перенаправление портов
Редирект с 80 на 8080 порт (TCP):
iptables -t nat -I PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080
Проверка:
iptables -L -nv -t nat

Сохранение конфигурации iptables
Сохранение и восстановление из файла
iptables-save > iptables.rules
iptables-restore < iptables.rules
Сервис netfilter-persistent для автоматического восстановления при загрузке системы
apt install iptables-persistent netfilter-persistent
netfilter-persistent save
Конфигурация в /etc/iptables

В качестве дополнительного материала рекомендую ознакомиться с альтернативой iptables: утилита UFW. https://losst.pro/nastrojka-ufw-ubuntu

apt install nginx apache2 libapache2-mod-php8.1 php8.1 php8.1-fpm mysql-server-8.0 - установка всех компонентов для работы веб серверов и базы данных mysql

Консольные утилиты для веб
Получить URL в консоли:
curl -L https://ya.ru/ - вывод кода страниы
wget https://yastatic.net/jquery/2.1.4/jquery.min.js - скачать файл в текущую папку

ps afx - Диспетчер задач

nginx -t - проверка конфигураии веб-сервера на синтаксис и прочее
apachectl -t

systemctl reload nginx - перезапуск сервера
nginx -s reload - перезагрузка с проверкой конфигураии

systemctl start apache2 - запуск сервера

systemctl status apache2 - проверка состояния сервера

РАБОТА С БАЗАМИ ДАННЫХ mysql:
mysql - запуск и вход для работы с базами данных mysql (пользователь root или права)

CREATE DATABASE mordvin_database; - создание базы данных
USE mordvin_database; - вход для работы с базой данных
CREATE TABLE mordvin_table(i INT); - создание таблицы

## Установка MySQL и первые шаги
● Установка: apt install mysql-server-8.0
● Заходим в консоль MySQL: sudo mysql
● Переходим в системную БД mysql: use mysql;
● Получаем список пользователей: SELECT * FROM user\G
● Создаём новую базу данных: CREATE DATABASE gb;
● Создаём таблицу: CREATE TABLE test(i INT);
● Создадим записи в таблице: INSERT INTO test (i) VALUES (1),(2),(3),(4);
● Сделаем выборку из таблицы: SELECT * FROM test;

apt install docker.io docker-compose - установка докера и докер-компоус (Запуск веб-приложения из контейнеров)

service nginx stop - остановить сервер (либо systemctl)

curl -L localhost:8082 - зайти на сайт с консоли

docker ps - диспетчер задач докера

docker run --rm --network compose-network --name mordvin-mariadb -e MYSQL_ROOT_PASSWORD=Mordvin00 -d mariadb:10.8

docker run --name myadmin -d --network compose-network --link mordvin-mariadb:db -p 8082:80 phpmyadmin - запуск с параметрами

docker network create compose-network - создание сетевого интерфейса

ss -ntlp | grep 80 - проверка соединений с филтром по 80

docker-compose ps - диспетчер задач

docker-compose up -d - запуск

apt install tree
tree - вывод древа папки и файлов

## Скрипты Bash

Bash — язык программирования
● Интерпретатор — /bin/bash
● Переменные
● Условия
● Циклы
● Функции
● Однострочные скрипты

Код возврата (завершения) — exit code
● Код ошибки последней команды
● Проверить: echo $?
● Условная связка (И): ls -al && echo "Success!"
● Условная связка (ИЛИ): ls -al || echo "Fail!"

Перенаправление потоков ввода-вывода
● program < file – перенаправление ввода из файла file
● program > file – перенаправление вывода (STDOUT) в файл file (запись с начала файла)
● program >> file – перенаправление вывода (STDOUT) в файл file в режиме дополнения файла
● program 2> file – перенаправление ошибок (STDERR) в файл file (запись с начала файла)
● program 2>> file – перенаправление ошибок (STDERR) в файл file в режиме дополнения файла
● program > file 2>&1 – перенаправление вывода (STDOUT) и ошибок (STDERR) в файл file
(запись с начала файла)

Конвейер (pipeline, pipe)

● Перенаправление ввода-вывода между процессами
● ls -al | grep file
● ls -al | grep -P '\.[cs]+'
● cat /var/log/syslog | grep 'mysql' | grep -v 'file' | wc -l

Переменные и их классификация

● Переменные окружения
○ $PATH
○ $UID
○ $PWD
● Пользовательские переменные
○ var1=test
○ echo $var1
● Специальные переменные
○ $1…$9
○ $?

Создаём первый скрипт на bash

cat > testscript
#!/bin/bash
directory=$1
hidden_count=$(ls -A $directory | grep '^\.' | wc -l)
echo “Hidden files in $directory found: $hidden_count”

Методы запуска скрипта

● Относительный путь: ./testscript
● Абсолютный путь: /home/db/test/testscript
● Команда (должен быть в $PATH): testscript
● Через команду bash: bash testscript
● Первые три варианта требуют шебанг и права на исполнение

Однострочные скрипты

● Разделитель команд — ";"
● Удобны для выполнения в терминале
● Применимы все основные конструкции из bash
● apt update; apt upgrade; echo "Upgrade complete!"

Условия if и ветвления

Синтаксис:
if [ выражение ]
 then
 Действия, если выражение истинно
 else
 Действия в противоположном случае
fi
Пример:
if [ -e file_name ]
 then
 echo "true"
 else
 echo "false"
fi

Варианты условий

Операции сравнения строк
● = или == возвращает true (истина), если строки равны
● != возвращает true (истина), если строки не равны
● -z возвращает true (истина), если строка пуста
● -n возвращает true (истина), если строка не пуста
Операции проверки файлов
● -e возвращает true (истина), если файл существует (exists)
● -d возвращает true (истина), если каталог существует (directory)
Операции сравнения целых чисел (наиболее используемые)
● -eq возвращает true (истина), если числа равны (equals)
● -ne возвращает true (истина), если числа не равны (not equal)

Цикл for

Синтаксис:
for имя_переменной in значения
do
 тело_цикла
done
Примеры:
for h in {01..24}
do
echo $h
done
for (( c=1; c<=5; c++ ))
do
 echo "Попытка номер $c"
done

Цикл while

Цикл while
Синтаксис:
while [ условие ]
do
 Тело_цикла
done
Пример:
c=10
while [ $c -ge 0 ]
do
echo "Test"
let "c = c - 1"
done


# Контейнеризация

## Механизм пространства имен

sudo chroot forchroot/ /bin/bash создание нового рута в папке forchroot/
mkdir forchroot/bin/
cp /bin/bash forchroot/bin - но надо скоировать туда оболочку и библиотеки
ldd /bin/bash - просмотр необходимых библиотек
mkdir forchroot/lib
mkdir forchroot/lib64
cp /lib/x86_64-linux-gnu/libtinfo.so.6 /lib/x86_64-linux-gnu/libc.so.6 forchroot/lib/
cp /lib64/ld-linux-x86-64.so.2 forchroot/lib64/ - скопировать библиотеки
pwd - выводит полный путь от корневого каталога к текущему рабочему каталогу
И теперь:
sudo chroot forchroot/ /bin/bash создание нового рута в папке forchroot/
Кстати команды работать не будут, надо их копировать с библиотеками в новый рут
exit - выйти из чрута
whereis ls - узнать где лежит программа ls
whereis ls
ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz
Копируем ls
mkdir forchroot/usr
mkdir forchroot/usr/bin
cp /usr/bin/ls forchroot/usr/bin/
Копируем библиотеки для ls, предварительно узнав где они, командой ldd:
ldd /usr/bin/ls
        linux-vdso.so.1 (0x00007fffccfcc000)
        libselinux.so.1 => /lib/x86_64-linux-gnu/libselinux.so.1 (0x00007f1906d54000)
        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1906a00000)
        libpcre2-8.so.0 => /lib/x86_64-linux-gnu/libpcre2-8.so.0 (0x00007f1906cbd000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f1906db4000)
cp /lib/x86_64-linux-gnu/libselinux.so.1 /lib/x86_64-linux-gnu/libc.so.6 /lib/x86_64-linux-gnu/libpcre2-8.so.0 forchroot/lib/
cp /lib64/ld-linux-x86-64.so.2 forchroot/lib64/
всё, теперь заходим в чрут:
sudo chroot forchroot/ /bin/bash
и ls будет работать, но диалектные команды (ll) не будут работать.

cd /proc/$$/ns
$$ - переменная bash, в которой хранится PID текущего процесса (консоль в нашем случае, с которой выполняется команда cd /proc/$$/ns)
/proc - папка где хранится информация о процессах
/ns - name space (пространства имен) - папка где хранится информация о квоте - совокупности пространств имен (ограничений) для текущего проесса

unshare - команда запуска программы unshare для запрета проессу namespace-ов

sudo unshare --net --user --pid --fork --mount-proc /bin/bash - запуск процесса с ограничениями по сети, пользователям, не видит проессов, папки с информацией о проессах

## Механизмы контрольных групп

sudo apt install lxc debootstrap bridge-utils lxc-templates - установка LXC контейнера

sudo lxc-create -n test123 -t ubuntu -f - создание  LXC контейнера
/usr/share/doc/lxc/examples/lxc-veth.conf

lxc-start -d -n test123 - запуск LXC контейнера

top - диспетчер задач в режиме реального времени

free -m - состояние ОЗУ

Чтобы убедиться, что контейнер действительно создался и работает, можно подключиться к нему и посмотреть как и что происходит:
lxc-attach -n test123
Собственно, далее предлагается посмотреть на изменения в уже известной нам папке:
ls /sys/fs/cgroup
Почему и был выбран LXC? Вся конфигурация и работа максимально близка к работе с системными ресурсами. Для каждого нового контейнера будет создаваться отдельная директория с набором файлов:
ls /sys/fs/cgroup/lxc.payload.test123/
Теперь еще один интересный момент: заниматься выделением и ограничением ресурсов можно как напрямую - через файлы, описанные выше, так и через специальные команды lxc:
/var/lib/lxc/test123/config
В этот файл можно добавлять различные параметры. Например:
lxc.cgroup2.memory.max = 256M
lxc.start.auto = 1
Эти строки сделают два действия:
1. Ограничат память контейнера 256 мегабайтами. Проверить это можно внутри контейнера командой free -m.
2. Контейнер будет автоматически запускаться при старте системы. Проверить это можно командой
lxc-ls -f
В момент вывода мы увидим, что параметр “автозапуск” включен...

На данном этапе могут возникнуть вопросы, так как в интернете встречается много литературы и мануалов, которые предлагают ограничивать контейнеры несколько иным способом:
memory.limit_in_bytes 400
Этот вариант касается установки параметров для первой версии контрольных групп. И, опять же, большинство инструкций в интернете касаются именно настройки cgroup v1.

lxc-create -n mordvinLXCcont -t ubuntu - установил LXC контейнер

lxc-start -d -n mordvinLXCcont - запуск контейнера не заходя в него (-d)

lxc-attach -n mordvinLXCcont - зайти в контейнер

sudo lxc-cgroup mordvinLXCcont memory.max 256M - ограничить оперативку контейнеру, команда из основной ОС запускать

sudo lxc-ls - отобразить все установленные контейнеры

man lxc - справка

lxc-ls -f - проверка опии автозапуска

## Введение в Docker
### Основные команды в Docker

docker run -it ubuntu:23.10 bash - установка и запуск ubuntu версии 23.10 в контейнере и попадаем в оболочку bash
-it - интерактивный режим, после запуска зразу попадаем в контейнер для работы в нём
-d - режим демона, запускаем в фоновом режие и остаёмся в консоли хостовой машины

docker ps - список запущенных контейнеров
docker ps -a - список всех установленных контейнеров

docker start id_container - запуск установленного контейнера (можно имя контейнера указать вместо id)

docker exec -it id_container bash - зайти в запущенный контейнер в оболочку bash

exit - выход из контейнера

docker stop id_container - остановка контейнера

docker container rm id_container - удаление контейнера

### Установка базы данных mysql и работа в mysql

docker run -d --name mardbcont -v /home/mordvin/Documents/Containerization/mydb:/var/lib/mysql -e MARIADB_ROOT_PASSWORD=Mordvin00 mariadb:10.10.2 - запуск образа с базой данных, с указанием пароля для рута - иначе не возможно будет зайти в неё, а так же с расшаренной папкой с базами данных на хостовой машине
-v /home/mordvin/Documents/Containerization/mydb:/var/lib/mysql - параметр который расшаривает папку с базами данных в контейнере на хостовую машину
-e MARIADB_ROOT_PASSWORD=Mordvin00 - указание пароля для рута - иначе не возможно будет зайти

docker exec -it id_container bash - зайти в запущенный контейнер в оболочку bash

mysql -u root -p - заходим в базу данных (пользователь root, пароль - Mordvin00)

show databases; - показать все базы данных, при работе в mysql в конце команд ставим ;

CREATE DATABASE GB; - создать базу данных "GB"

USE GB; - зайти в базу данных

SELECT * FROM tablename; - показать всё содержимое таблицы tablename

exit - выход из mysql

postgreesql - альтернативная марии база данных

### Графическая оболочка управления базой данных phpmyadmin

docker run -d --name phpmyadmin --link mardbcont:db -p 8081:80 phpmyadmin/phpmyadmin - запуск графической оболочки, теперь можно зайти через веб-браузер по порту 8081 на хостовую машину по ip адресу или сетевому имени, на самой машине 127.0.0.1:8081 или localhost:8081
--link mardbcont:db - связать граф.оболочку с конт.базы данных mardbcont
-p 8081:80 - проброс портов

adminer - алтернативная оболочка управления базой данных


## Dockerfile и слои

### Инструкции Dockerfiles

💡 FROM
Задает базовый образ
💡 LABEL
Добавление метаданных в образ
💡 ENV
Задает постоянные переменные образа
💡 ARG
Задание переменных во время сборки образа
💡 RUN
Выполнение команды Linux
💡 COPY
Копирование файлов и папок
💡 ADD
Копирование файлов и папок + распаковка архивов
💡 WORKDIR
Задание рабочей директории
💡 CMD
Задание команды, выполняющейся при запуске образа
💡 ENTRYPOINT
Задание команды, выполняющейся при запуске образа
💡 EXPOSE
Открытие порта в образ/контейнер
💡 VOLUME
Создание точки монтирования

Инструкция FROM
💡 Каждый Dockerfile начинается с нее
💡 Определяет базовый образ, на основе которого будет строиться образ

Инструкция LABEL
💡 Добавляет в образ различные метаданные
💡 Не замедляет процесс сборки образа и не увеличивает его размер

Инструкция ENV
💡 Создает постоянные переменные в образе
💡 Подходит при необходимости использования констант внутри

Инструкция RUN
💡 Выполняет Linux-команду во время сборки образа
💡 В процессе выполнения создается отдельный слой

Инструкция COPY
💡 Копирует файлы и папки в процессе сборки образа
💡 В случае отсутствия директории, создает ее

Инструкция ADD
💡 Может решать те же задачи, что и COPY
💡 Добавление файлов из удаленных источников
💡 Распаковка tar-файлов

Инструкция WORKDIR
💡 Изменяет рабочую директорию контейнера
💡 Лучше указывать абсолютные пути при обращении к файлам
💡 В случае отсутствия директории, происходит автоматическое создание

Инструкция ARG
💡 Задает переменную, значение которой передается во время выполнения
💡 Имеет значение по умолчанию

Инструкция CMD
💡 Описывает команду, которая выполнится при запуске контейнера
Можно переопределить аргументы команды, передав при запуске
💡 Результат не добавляется в образ во время сборки!

Инструкция ENTRYPOINT
💡 Описывает команду и аргументы, которые будут выполнены сразу после
запуска контейнера
💡 Важное отличие от CMD: команду и аргументы нельзя переопределить

ENTRYPOINT vs CMD
💡 Выполнение одной и той же команды раз за разом
💡 Если контейнер используется в роли приложения
💡 Если постоянно необходимо передавать в контейнер различные аргументы

Инструкции EXPOSE и VOLUME
💡 Инструкция EXPOSE
Указывает на необходимость открытия портов для связи
💡 Инструкция VOLUME
Указывает место в локальной системе, которое контейнер будет использовать для хранения данных

### Пример записи докерфайла для создания и запуска контейнера:

ARG VERSION

FROM ubuntu:${VERSION}

RUN apt update && apt install nginx -y

LABEL "website.name" = "GB website by Mordvin"

ENV WEBPATH=/var/www/html

#COPY ./website /var/www/html
#COPY site.conf /etc/nginx/conf.d/site.conf

WORKDIR /var/www

EXPOSE 80 443

VOLUME /var/www

CMD ["nginx", "-g", "daemon off;"]

### Пример запуска контейнера по созданному файлу с инструкиями:

docker build -t mordvinnginximage --build-arg VERSION=23.10 .

docker run -d -p 8081:80 mordvinnginximage


## Docker Compose и Docker Swarm

Основные команды ДК
Прежде чем мы окончательно перейдем к практике, предлагаю рассмотреть
набор доступных команд. Он невелик и достаточно прост:
● docker-compose build - команда позволяет собрать сервисы, описанные в
конфигурационных файлах
● docker-compose up -d - запускает наш проект. В данном случае проект
запустится в фоновом режиме, так как в команде присутствует флаг -d
● docker-compose start - запускает любые остановленные ранее сервисы в
соответствии с указанными параметрами
● docker-compose down - останавливает наш проект и, что немаловажно,
удаляет все сервисы, которые были запущены ранее
● docker-compose stop - эта команда просто останавливает все сервисы,
описанные в конфигурации. Она не удаляет контейнеры, тома, сети и прочие
сущности, описанные в конфигурационном файле
● docker-compose logs -f [service name] - с помощью этой команды можно
посмотреть логи нашего сервиса
● docker-compose ps - выводит на экран список всех доступных контейнеров
● docker-compose exec [service name] [command] - с ее помощью можно
выполнить команду в сервисе, не заходя при этом в контейнер. Ранее мы
рассматривали подобное на уроке “Введение в Docker”
● docker-compose images - позволяет вывести список образов.
10
Описание проекта
Итак, разобравшись с теорией, самое время перейти к практике. Начнем ее с
уже известного нам манифеста:

version: ‘3.9’ - версию уже можно не казывать, так как разработчики добились стабильной работы и совместимости
services:
db:
image: mariadb:10.10.2
#как пример показать и такое в режиме лайвкода:
build: ./db
restart: always
environment:
MYSQL_ROOT_PASSWORD: 12345
adminer:
image: adminer:4.8.1
restart: always
ports:
- 6080:8080
В целом, манифест довольно прост. Давайте перейдем к комментариям и
описанию:
● version: ‘3.9’ - в этой строке описывается версия compose-файла. Каждый
файл обязательно должен начинаться с тега версии. Глобально версий всего
три. Первая, которая более не поддерживается, вторая и третья. Каждой
версии файла соответствует определенная версия докера, установленная на
машине. Например, для версии 3.1 необходимо, чтобы установленный докер
был не ниже версии 1.13.1, а вот версия 3.8 уже требует минимум версию
19.03.0.
11
● services - эта строка говорит о том, что далее в файле будут описаны сервисы
(один или несколько), которые необходимо запустить или остановить
(зависит от команды, передаваемой в докер). Важно помнить, что ДК
работает с сервисами (1 сервис = 1 контейнер). Сервисом при этом может
быть клиент, сервер, сервер БД и так далее.
● db: - это название первого сервиса. Название каждый может придумать какое
угодно вам. Понятное название сервиса поможет определить его роль и
сделать манифесты более читаемыми.
● build: ./db - ключевое слово, позволяющее задать путь к файлу докерфайл,
который будет использован для создания собственного образа и который, в
свою очередь, позволит запустить сервис.
● image: mariadb - если вместо build указан этот вариант, система будет
использовать готовые образы с docker hub. На лекции, в примерах, мы будем
использовать этот вариант. На практике попробуем разобрать варианты с
построением собственных образов.
● restart: always - позволяет определить политику перезапуска контейнера.
Существует несколько вариантов:
○ no - не перезапускать контейнер автоматически. Это значение по
умолчанию. Если мы ничего не укажем, контейнер не будет
перезапускаться
○ on-failure [max retries] - перезапускать контейнер, если он был
завершен не с нулевым кодом выхода. В данном случае необходимо
указать максимальное количество попыток перезапуска
○ always - всегда перезапускать контейнер, если он был остановлен.
Здесь стоит быть аккуратнее в связи с тем, что в этом случае возможен
цикличный перезапуск контейнера с проблемами
○ unless-stopped - всегда перезапускать контейнер. По сути, этот вариант
аналогичен always кроме случаев, когда он был остановлен вручную.
При этом он не перезапускается после перезапуска демона докера.
○ environment - в этом блоке прописываются переменные, которые могут
быть использованы для работы контейнера
○ ports - указывает на то, какие порты необходимо открыть для связи с
контейнером
12
Немного практики
Итак, ознакомившись с проектом, можно приступить к практике.
Первым делом проверим, что у нас ничего не запущено:
docker ps -a
root@testVM:~/lesson5# docker images
Отлично, давайте запустим наш проект. Для этого перейдем в директорию с
нашим yml-файлом и запустим:
docker-compose up
Как мы видим, процесс пошел:
docker-compose up
Creating network "lesson5_default" with the default driver
Pulling db (mariadb:10.10.2)...
10.10.2: Pulling from library/mariadb
039ecd174df7: Pull complete
f9e5f484f6bc: Pull complete
Digest:
sha256:940985c1cf37812ffb3bb6c7b34b4e40233e0907fc786ec7d63c49553d
7d1454
Status: Downloaded newer image for mariadb:10.10.2
Pulling adminer (adminer:4.8.1)...
4.8.1: Pulling from library/adminer
be31139c5821: Pull complete
d183076dd611: Pull complete
Digest:
sha256:3b4e25b39404729b27bbb2895fa0a8fefe8ec19cdad1fe85c85500f4c0
80c7f4
Status: Downloaded newer image for adminer:4.8.1
13
Сначала произошло скачивание первого образа (который с БД), затем второго.
Ну и далее, как можем видеть, запустилась наша БД и adminer. Учитывая, что мы
запускали приложения не в фоновом режиме, они заняли нашу консоль.
Итак, как же все это произошло? Теперь можем обсудить последствия
выполнения нашего манифеста.
Первое - были скачаны два различных образа из официального репозитория.
Это довольно простая операция и тут все должно быть предельно ясно.
Второе - из образов были созданы контейнеры. В целом, тоже ничего
сложного.
Третье - оба этих контейнера должны общаться друг с другом. Это также
значит, что они формально имеют какую-либо общую сетевую инфраструктуру. Это
может показаться сложным, однако, сейчас давайте поверим на слово, что
инфраструктура (то бишь сеть) у них общая. Оба контейнера получили свои
IP-адреса. Этот момент давайте проверим на всякий случай. Для этого запустим
замечательную команду:
docker container inspect adminer
docker container inspect mariadb
Как видим, контейнеры имеют корректные адреса. Значит, действительно,
сеть у них есть и эта сеть - общая. О тонкостях работы сетевых интерфейсов мы
поговорим немного позднее.
А сейчас же давайте насладимся результатом работы! Для этого мы перейдем в
браузер и попробуем посмотреть на результат нашей работы. Важно отметить, что
мы выполнили проброс портов. То есть, мы имеем доступ внутрь контейнера через
сетевой интерфейс хостовой машины. Далее вводим адрес нашей машины вместе с
портом и видим приветственное окно adminer:

Вводим логин и пароль, который мы указывали (root/12345) и получаем вход в
нашу БД.
Однако, есть проблема. Мы запустили наши контейнеры не в фоновом режиме.
Давайте несколько изменим команду, добавив флаг -d:
docker-compose up -d
Starting lesson5_adminer_1 ... done
Starting lesson5_db_1 ... done
Теперь все запущено и работает. Можем проверить это. Нужно заострить
внимание в этом случае на именовании запущенных контейнеров.
docker ps
CONTAINER ID IMAGE COMMAND CREATED
STATUS PORTS NAMES
bbf5aff9e554 adminer:4.8.1 "entrypoint.sh docke…" 9
minutes ago Up 45 seconds 0.0.0.0:6080->8080/tcp,
:::6080->8080/tcp lesson5_adminer_1
3e6fb7a7f0e4 mariadb:10.10.2 "docker-entrypoint.s…" 9
minutes ago Up 46 seconds 3306/tcp
lesson5_db_1
15
Теперь все отлично работает и никак не мешает нам. Давайте рассмотрим ряд
описанных выше команд:
docker-compose stop #здесь показать, что контейнеры были
остановлены, но не удалены!
docker ps -a #отображаем остановленные контейнеры
docker-compose start -d #снова запускаем и проверяем, что все
хорошо
docker-compose logs #показать, что логи тоже есть
Start existing containers. #заострить внимание на этом!
docker ps -a
docker-compose down #показать, что в этом случае они удалятся
docker ps -a #проверяем удаление
docker-compose start #пытаемся запустить удаленное и получаем
ошибку
Starting db ... failed
Starting adminer ... failed
ERROR: No containers to start
ERROR: 1
Что такое Docker Swarm (ДС)
Итак, деплой приложения, состоящего из нескольких контейнеров мы освоили.
Теперь давайте добавим к этому отказоустойчивость! Давайте научим наш докер
работать не на одном сервере (ноде), а на трех, что принесет несколько плюсов:
1. Отказоустойчивость - при выходе из строя одной ноды из трех наши
контейнеры смогут продолжать работу на остальных узлах кластера.
2. Увеличение ресурсов - в случае работы в кластере, существенно
увеличивается мощность, доступная контейнерам для работы.
Прежде чем мы продолжим, давайте дадим несколько определений, которыми
будем оперировать в дальнейшем.
Node (нода) - это наш сервер с установленным на нем Docker. По сути, нодой
могут быть как физические сервера, так и виртуальные машины. На лекционных и
16
семинарских занятиях в качестве нод мы и будем использовать виртуальные
машины - их мощностей более чем достаточно для наших примеров.
Stack - это набор сервисов, которые могут быть связаны между собой
логически. Иными словами, это набор сервисов, которые описываются в обычном
compose файле. Важно заметить, что части стека могут располагаться как на одной
ноде, так и на различных.

Сервис - это составляющая стека. Сервис является описанием того, какие
контейнеры необходимо создать. Ранее мы уже рассмотрели ДК файлы и, по сути,
на практике познакомились с этой сущностью.
Task (задача) - непосредственно созданный контейнер, который движок
докера создает на основе предоставленной информации.
Итак, разобравшись с набором терминов, предлагаю перейти непосредственно
к знакомству.
ДС - это стандартный оркестратор для контейнеров, который изначально
встроен в механизм работы докера. Он отлично подойдет для развертывания
приложений в рабочей среде кластера. Его несомненный плюс - не нужно будет
переписывать наши ДК файлы, а просто применять их для работы с ДС.
Также хочу заметить, что у ДС есть два типа нод (узлов):
● manager node - управляющий сервер. Он способен управлять нашим
кластером: добавлять и удалять сущности (например, ноды кластера). На нем
также возможно запускать и контейнеры.
● worker node - на этом узле возможен лишь запуск контейнеров. Управление
кластера с данного типа узлов недоступен.
17
Существует несколько вариантов делегации прав. Если вы строите кластер из
3 нод, можно сделать только одну ноду управляющей, а оставшиеся две -
управляемыми с точки зрения безопасности и оградить управляющую ноду от
лишних соединений извне. С другой стороны, если наш кластер содержится в
отдельной сети, к которой злоумышленники наверняка не получат доступ, можно
сделать все три ноды управляющими. Это позволит управлять кластером с любой
ноды, а также, в случае выхода из строя одной из нод, не будет теряться
управление кластером.
Вторая вариация: 5-нодовый кластер. В данном случае можно сделать 2 или 3
ноды управляющими, а остальные - рабочими. На самом деле это дело вкуса.
Каждый конфигурирует кластер так, как считает нужным и как ему это удобно.
В случае, если вы хотите развернуть большое производство с сотнями
контейнеров и общим хранилищем, оптимально рассмотреть как минимум второй
вариант.
Итак, для создания нашего кластера необходимо установить компоненты
докера. Их мы установили ранее и приступим непосредственно к созданию
кластера.
Теперь давайте инициализируем наш кластер:
docker swarm init
В ответ на это мы получили следующую команду:
docker swarm join --token
SWMTKN-1-3un77cn4m5ok3ijrdouwg3mit69uwmfwx96krc7taua7ovpjha-97y4z
k9ppc8hxk2caxlig23xo 192.168.50.90:2377
Это команда, с помощью которой мы сможем присоединить другие узлы
нашего будущего кластера. По сути, ввести их в кластер. На всех этих узлах
необходимо ее выполнить.
Давайте это и сделаем!
Теперь, когда все команды выполнены успешно, давайте проверим состояние
нашего кластера:
docker node ls
ID HOSTNAME STATUS
AVAILABILITY MANAGER STATUS ENGINE VERSION
u6v4f0i3n88d4ab71m3nlolw8 * docker-1 Ready Active
Leader 20.10.21
zjgj80mu5gk1sjlclfx41dwxh docker-2 Ready Active
18
20.10.21
xeh4kycewpk780ytv6zjqvl06 docker-3 Ready Active
20.10.21
Итак, как мы видим, в кластере отображается ID каждой ноды, ее hostname и
еще ряд информации: доступна ли она в настоящий момент времени и в каком
статусе преобладает, готова ли она к запуску контейнеров или нет. Также, в самой
правой колонке отображается информация о типе ноды manager или work нода в
кластере.
Если вы думали, что инициализация кластера - сложное дело, то вы
ошибались. Итак, наш кластер создан и готов к работе!
Однако, у нас есть еще одна нода. Давайте добавим и ее.
docker swarm join --token
SWMTKN-1-3un77cn4m5ok3ijrdouwg3mit69uwmfwx96krc7taua7ovpjha-97y4z
k9ppc8hxk2caxlig23xo 192.168.50.90:2377
docker node ls
Она необходима будет для демонстрации вывода из кластера при различных
условиях.
Отлично, она добавлена. Теперь давайте выведем ее из кластера:
docker swarm leave
Теперь давайте посмотрим, в каком статусе содержится нода:
docker node ls # необходимо немного времени
Как можно видеть, нода теперь имеет статус не ready, а down. С этого момента
ДС не будет использовать ее для размещения контейнеров и можно спокойно далее
эксплуатировать кластер, не боясь за сохранность содержимого.
Давайте попытаемся повторно ввести ноду в кластер и посмотрим, что из этого
выйдет.
Для того, чтобы можно было окончательно удалить эту ноду, нужно выполнить
на менеджере:
docker node rm (ID ноды)
Теперь у нас нет задвоения нод в списке и все в порядке. Давайте продолжим
дальше.
19
Как можно видеть, у нас только одна нода в статусе менеджера. Давайте
увеличим это количество до двух:
docker node promote docker-2
root@docker-1:~# docker node promote docker-2
Node docker-2 promoted to a manager in the swarm.
root@docker-1:~# docker node ls
ID HOSTNAME STATUS
AVAILABILITY MANAGER STATUS ENGINE VERSION
u6v4f0i3n88d4ab71m3nlolw8 * docker-1 Ready Active
Leader 20.10.21
zjgj80mu5gk1sjlclfx41dwxh docker-2 Ready Active
Reachable 20.10.21
x2hc3z3irk25mi8cd7dcwyxm0 docker-3 Ready Active
20.10.21
Теперь же состояние второй ноды изменилось, однако, оно стало не leader, как
можно было подумать, а reachable. Формально, она стала запасным лидером.
Далее, что же можно еще сделать с нашими нодами? Давайте введем
следующую команду и посмотрим на предложения:
docker node --help
Usage: docker node COMMAND
Manage Swarm nodes
Commands:
demote Demote one or more nodes from manager in the
swarm
inspect Display detailed information on one or more
nodes
ls List nodes in the swarm
promote Promote one or more nodes to manager in the
swarm
ps List tasks running on one or more nodes,
defaults to current node
rm Remove one or more nodes from the swarm
update Update a node
Давайте рассмотрим, какая информация о ноде содержится в кластере:
docker node inspect
20
Docker Swarm и Overlay сети
Итак, с нодами мы разобрались. Теперь пришло время разобраться еще с
одной сущностью, которая также важна в организации кластера - сети.
Первое, о чем может быть вопрос - зачем создавать дополнительные сети,
если итак все работает?! И это правда. Можно ничего не знать об этой сущности и
использовать инструмент “как есть”, однако, существуют некоторые тонкости, о
которых пойдет речь далее.
Overlay-сети используются в кластерах ДС, где виртуальная сеть, используемая
контейнерами, связывает несколько физических хостов, на которых запущен
Docker. В случае, когда мы запускаем контейнер на swarm-кластере (как часть
сервиса), множество сетей присоединяется по умолчанию, и каждая из них
соответствует разным требованиям связи.
Также хочу отметить, что существует несколько типов этих сетей:
● Overlay - создает простую подсеть, которая может быть использована
контейнерами на разных хостах swarm-кластера. В этом случае контейнеры,
которые располагаются на разных физических хостах будут обмениваться
именно через overlay сеть. Важное условие - каждый компонент должен
иметь доступ к такой сети. Технически при выборе данной сети, мы получаем
дополнительный слой
● ingress - этот тип сети используется в ДС по умолчанию при создании
кластера. Она отвечает за связи, которые устанавливаются между
контейнерами (то есть сущностями внутри кластера и внешним миром).
Посредством этой сети происходит балансировка нагрузки, которую по
умолчанию и предоставляет ДС кластер.
● vxlan - при использовании этого типа сетей, у нас не просто происходит
создание Overlay-сети. В этом случае происходит инкапсуляция пакетов 2
слоя модели OSI в четвертый. С помощью этого действия докер и создает
этот тип сети. Любые конечные точки, содержащие этот тип сети, видят друг
друга так, будто они подключены друг к другу посредством одного свитча.
Эта сеть является технологией сетевой виртуализации, созданной для
решения проблем масштабируемости в больших системах облачных
вычислений.
● docker_gwbridge - эта сеть создается на каждом узле кластера. Она позволяет
соединить трафик из контейнеров, находящихся внутри ДС кластера с
21
внешним миром. Например, в случае, если мы из контейнера запустим
команду ping ya.ru, будет использована эта сеть.
Советы по использованию ДК и ДС
Итак, ознакомившись с основами и поняв, как работает ДК, можно перейти к
ряду более глобальных советов по его использованию. Есть несколько общих
рекомендаций, которых было бы здорово придерживаться при использовании этого
инструмента. Давайте о них напоследок и поговорим.
Совет 1: используйте виртуальные сети для
различных проектов
Этот момент очевидный, но мало кто к нему сразу приходит. Связано это с тем,
что “из коробки” все и так работает. Использование сети хоста не рекомендовано в
связи с тем, что, резервируя какой-либо порт одним контейнером, вы его занимаете
и не можете использовать повторно.
Это чревато тем, что другие приложения использовать его не смогут и вы,
например, не сможете запустить две версии своего контейнеризированного
приложения. А если у вас есть разное ПО, которое при этом использует схожие
порты - вам вдвойне не повезло, так как искать причину неработоспособности вы
будете довольно долго, не зная о существовании виртуальных сетей.
Вы можете спросить: “Какие еще преимущества дает использование
виртуальных сетей?”
А их не так уж и мало:
1. Изолированная сеть от хоста и остальных приложений. Это значит, что в сети
не будет содержаться никакого паразитного трафика, который может
повлиять тем или иным способом на работу ваших приложений. Также в этом
случае крайне маловероятно, что особенности нашего хостового окружения
приведут к изменению поведения сборки нашего контейнеризированного
ПО.
2. В случае использования основной сети (хостовой), контейнеры
прослушивают адрес 0.0.0.0. Если же мы открываем какой-либо порт для
контейнера, то этот же порт открывается и у хоста, а это уже серьезная брешь
в безопасности! В случае использования виртуальной сети таких проблем
также не возникнет.
22
3. При использовании докера и его компонентов, процессы (контейнеры) могут
обмениваться данными друг с другом не только по IP-адресу, но и по имени.
Используя виртуальные сети, можно ограничить подобного рода обращения,
можно развернуть рядом несколько окружений (dev, stage, prod) без
какой-либо вероятности конфликтов между окружениями.
Совет 2: Корректно открывать порты, не используя
адрес 0.0.0.0
Эту ошибку мы допустили и на этом уроке, и на предыдущих. Мы открывали
порты, записывая их как 8080:80. В целом это выглядит обычно и очень даже
безобидно, однако, дьявол кроется в деталях. Эта привязка довольно часто
встречается в мануалах и, в общем, сама по себе корректна. Однако, она не просто
будет перенаправлять порт с контейнера на локальный хост, она перенаправит его
на хост и сделает доступным этот порт абсолютно на каждом сетевом интерфейсе
нашей системы, включая и те, которые имеют доступ в сеть интернет. Сами
понимаете, что это довольно серьезная брешь в безопасности. Если мы делаем
какие-то небольшие проекты у себя дома, находясь за роутером, который защитит
от подобного, то проблема не столь существенна и может быть проигнорирована.
Однако, если мы находимся в корпоративной сети или хуже - наш сервер имеет
прямой выход в интернет - все куда сложнее.
Да, разумеется, можно продолжить использовать эту простую запись и плохого
ничего не будет. Но для этого необходимо корректно настраивать фаервол на
сервере, либо на маршрутизаторе, чтобы злоумышленник не смог воспользоваться
этой лазейкой.
Сетевые сущности на практике
Итак, рассмотрев теорию, давайте перейдем к практике. Начнем с самого
простого - с просмотра имеющихся у нас сетей в кластере:
docker network ls
NETWORK ID NAME DRIVER SCOPE
d857a206d442 bridge bridge local
ee1cd270bcc9 docker_gwbridge bridge local
06feb6eea007 host host local
1iwj1bt3rhus ingress overlay swarm
23
c357f8c1dbcd none null local
На этом моменте важно заметить, что ID сети уникален, однако, на каждой
ноде кластера имеется только одна сеть, которая имеет один и тот же ID - Ingress.
Теперь же давайте создадим нашу сеть с помощью следующей команды:
docker network create --driver overlay --subnet 4.5.6.0/24
test-network --attachable
b8y23nyh0m8yu7y35vw890bl1
root@docker-1:~# docker network ls
NETWORK ID NAME DRIVER SCOPE
d857a206d442 bridge bridge local
ee1cd270bcc9 docker_gwbridge bridge local
06feb6eea007 host host local
1iwj1bt3rhus ingress overlay swarm
c357f8c1dbcd none null local
b8y23nyh0m8y test-network overlay swarm
Давайте проверим, что сеть доступна и с другой ноды кластера.
Смотрите, ID нашей сети также остается одним и тем же на каждой ноде кластера.
Итак, идем дальше. Давайте теперь запустим контейнер и подключим его к нашей
сети.
docker run -d --ip 4.5.6.7 --net test-network --name
container-1 busybox sleep 3600
Как можем видеть, контейнер был успешно запущен. Давайте следом запустим
второй, но на другой ноде:
docker run -d --ip 4.5.6.8 --net test-network --name
container-2 busybox sleep 3600
Теперь же давайте войдем внутрь первого контейнера и выполним проверку
доступности.

Как войти внутрь контейнера?
Ответ:
docker exec -it container-1 sh
Теперь давайте проверим, что контейнеры между собой могут общаться и
доступны:
ping 4.5.6.7
PING 4.5.6.7 (4.5.6.7): 56 data bytes
25
64 bytes from 4.5.6.7: seq=0 ttl=64 time=0.160 ms
64 bytes from 4.5.6.7: seq=1 ttl=64 time=0.137 ms
^C
--- 4.5.6.7 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max = 0.137/0.148/0.160 ms
/ # ping 4.5.6.8
PING 4.5.6.8 (4.5.6.8): 56 data bytes
64 bytes from 4.5.6.8: seq=0 ttl=64 time=0.850 ms
64 bytes from 4.5.6.8: seq=1 ttl=64 time=0.407 ms
Как можно видеть, из первого контейнера доступны оба адреса: его и второго
контейнера, который при этом расположен на совершенно другой ноде.
Теперь давайте попытаемся понять несколько больше: выйдем из этого
контейнера.
Теперь мы снова на хосте. Давайте повторим пинг контейнеров. Как можно
видеть, оба недоступны. У нас ничего не получилось в связи с тем, что сам хост
абсолютно ничего не знает об адресации 4.5.6.0/24.
Окей, вроде бы стало понятно: как контейнеры работают друг с другом,
однако, каким образом происходит связь оверлейной сети и реальных (физических)
адаптеров?
Давайте попробуем это понять. Для этого давайте посмотрим на доступные
сетевые пространства имен:
root@docker-1:~# ll /var/run/docker/netns
total 0
drwxr-xr-x 2 root root 140 Nov 29 07:23 ./
drwx------ 8 root root 180 Nov 29 07:23 ../
-r--r--r-- 1 root root 0 Nov 29 06:58 1-1iwj1bt3rh
-r--r--r-- 1 root root 0 Nov 29 07:23 1-q8ns9akstq
26
-r--r--r-- 1 root root 0 Nov 29 07:23 a205613434af
-r--r--r-- 1 root root 0 Nov 29 06:58 ingress_sbox
-r--r--r-- 1 root root 0 Nov 29 07:23 lb_q8ns9akst
Здесь нас интересует наша оверлейная сеть. Давайте попробуем побольше
узнать о ней:
nsenter --net=/var/run/docker/netns/1-q8ns9akstq ip -d link
show
В пространстве имен оверлей сети содержится три интерфейса (вместе с lo):
● br0: бридж
● veth2: виртуальный интерфейс veth, который связан с интерфейсом eth0 в
нашем контейнере и который подключен к бриджу.
● vxlan0: интерфейс типа «vxlan», который также подключен к бриджу
Собственно, на этом моменте становится понятнее: в эту сеть подключен
виртуальный адаптер, который и связывает эту сеть с сетью хоста. Благодаря этому
подключению и возможна связь между контейнерами узла.
На практике мы подробнее изучим запуск контейнеров из наборов
конфигурационных файлов. Также изучим более подробно работу виртуальных
сетей и аспекты работы в кластере.
Заключение
Итак, вас можно поздравить! Вы стали на шаг ближе к цели. Это последняя
лекция из курса “Основы контейнеризации”. Крайне рекомендую вам продолжить
обучение и изучить курс “Продвинутая контейнеризация”. Там будут изучаться
аспекты работы с Kubernetes и другие интересные моменты.

### docker-compose.yml

services:
  db:
    image: mariadb:10.10.2
#    build: ./db
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: Mordvin00
    volumes:
      - db:/var/lib/mysql
    networks:
      - mord-network
    deploy:
      mode: replicated
      replicas: 2

  adminer:
    depends_on:
      - db
    image: adminer:4.8.1
    restart: unless-stopped
    ports:
      - 6080:8080
    volumes:
      - ./mordadminer:/mordadminer
    networks:
      - mord-network
    deploy:
      mode: replicated
      replicas: 1

volumes:
    db:

networks:
  mord-network:
    driver: bridge